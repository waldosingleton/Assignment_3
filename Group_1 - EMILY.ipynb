{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BBC News Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Exploratory Data Analytics\n",
    "###### (a) Load the dataset and construct a feature vector for each article in the. You need to report the number of articles, and the number of extracted features. Show 5 example articles with their extracted features using a dataframe.\n",
    "###### (b) Conduct term frequency analysis and report three plots: (i) top-50 term frequency distribution across the entire dataset, (ii) term frequency distribution for respective class of articles, and (iii) class distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleId</th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1821</td>\n",
       "      <td>johnny denise lose passport johnny vaughan den...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>522</td>\n",
       "      <td>bt offers free net phone calls bt offering cus...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>636</td>\n",
       "      <td>power people says hp digital revolution focuse...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>170</td>\n",
       "      <td>stars gear bafta ceremony film stars across gl...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>85</td>\n",
       "      <td>controversial film tops festival controversial...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ArticleId                                               Text       Category\n",
       "0      1821  johnny denise lose passport johnny vaughan den...  entertainment\n",
       "1       522  bt offers free net phone calls bt offering cus...           tech\n",
       "2       636  power people says hp digital revolution focuse...           tech\n",
       "3       170  stars gear bafta ceremony film stars across gl...  entertainment\n",
       "4        85  controversial film tops festival controversial...  entertainment"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"train.csv\", skiprows=0, header=0, na_values= \"\", dtype=str)\n",
    "df.head()\n",
    "\n",
    "## Load Test Data ##\n",
    "testdf = pd.read_csv(\"test.csv\", skiprows=0, header=0, na_values= \"\", dtype=str)\n",
    "testdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 1\n",
      "article vector\n",
      " [[0 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "\n",
      "article vector (5 articles)\n",
      " [[0 0 0 ... 0 0 0]\n",
      " [0 0 1 ... 1 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " [1 0 0 ... 0 0 1]]\n",
      "\n",
      " Method 2\n",
      "article vector\n",
      " [[0.         0.02011467 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "\n",
      "article vector (5 articles)\n",
      " [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.08140554 ... 0.08140554 0.         0.        ]\n",
      " [0.         0.059516   0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.0722941  0.        ]\n",
      " [0.06848469 0.         0.         ... 0.         0.         0.06848469]]\n",
      "\n",
      "Articles: 428 , Extracted Features: 13518\n"
     ]
    }
   ],
   "source": [
    "articles_text = df[\"Text\"].to_numpy()\n",
    "\n",
    "#select 5 random articles for task 1\n",
    "random_sample = random.sample(list(articles_text), 5)\n",
    "\n",
    "## APPROACH ONE ##\n",
    "vectorizer1 = CountVectorizer()\n",
    "vectorizer1.fit(articles_text)\n",
    "\n",
    "vectorizer1_sample = CountVectorizer()\n",
    "vectorizer1_sample.fit(random_sample)\n",
    "\n",
    "#Summary\n",
    "#print(f'vector vocabulary - {vectorizer.vocabulary_}\\n')\n",
    "\n",
    "# encode document\n",
    "vector1 = vectorizer1.transform(articles_text)\n",
    "vector1_sample = vectorizer1_sample.transform(random_sample)\n",
    "\n",
    "# summarize encoded vector\n",
    "print(\"Method 1\")\n",
    "print(f'article vector\\n {vector1.toarray()}')\n",
    "print(f'\\narticle vector (5 articles)\\n {vector1_sample.toarray()}')\n",
    "\n",
    "## APPROACH TWO ##\n",
    "vectorizer2 = TfidfVectorizer()\n",
    "vectorizer2.fit(articles_text)\n",
    "\n",
    "vectorizer2_sample = TfidfVectorizer()\n",
    "vectorizer2_sample.fit(random_sample)\n",
    "\n",
    "#Summary\n",
    "#print(f'vector vocabulary - {vectorizer2.vocabulary_}\\n')\n",
    "\n",
    "# encode document\n",
    "vector2 = vectorizer2.fit_transform(articles_text)\n",
    "vector2_sample = vectorizer2_sample.transform(random_sample)\n",
    "\n",
    "# summarize encoded vector\n",
    "print('\\n', \"Method 2\")\n",
    "print(f'article vector\\n {vector2.toarray()}')\n",
    "print(f'\\narticle vector (5 articles)\\n {vector2_sample.toarray()}')\n",
    "print('\\nArticles:', vector2.shape[0], ', Extracted Features:', vector2.shape[1])\n",
    "\n",
    "# for my task\n",
    "## APPROACH ONE ##\n",
    "vectorizer1 = CountVectorizer()\n",
    "vectorizer1.fit(articles_text)\n",
    "\n",
    "vectorizer1_sample = CountVectorizer()\n",
    "vectorizer1_sample.fit(random_sample)\n",
    "\n",
    "# encode document\n",
    "vector1 = vectorizer1.transform(articles_text)\n",
    "vector1 = vector1.toarray()\n",
    "v1_names = vectorizer1.get_feature_names_out()\n",
    "\n",
    "vector1_sample = vectorizer1_sample.transform(random_sample)\n",
    "vector1_sample = vector1_sample.toarray()\n",
    "v1_sample_names = vectorizer1_sample.get_feature_names_out()\n",
    "\n",
    "#map sample to feature names\n",
    "v1_sample_df = pd.DataFrame(data=zip(v1_sample_names, vector1_sample[0], vector1_sample[1], vector1_sample[2], vector1_sample[3], vector1_sample[4], sum([vector1_sample[0], vector1_sample[1], vector1_sample[2], vector1_sample[3], vector1_sample[4]])), columns=['Features', 'Article 1', 'Article 2', 'Article 3', 'Article 4', 'Article 5', 'Total'])\n",
    "\n",
    "v1_df_emily = pd.DataFrame(data=vector1, columns = v1_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Classification Models Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "###### Train your logistic regression classifier with L2-regularization. Consider different values of the regularization term Œª. Describe the effect of the regularization parameter Œª on the outcome in terms of bias and variance. Report the plot generated for specific Œª values with training loss on the y-axis versus Œª on the x-axis to support your claim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Logistic Regression Predictions\n",
      "        Prediction\n",
      "0    entertainment\n",
      "1             tech\n",
      "2             tech\n",
      "3    entertainment\n",
      "4    entertainment\n",
      "..             ...\n",
      "101  entertainment\n",
      "102  entertainment\n",
      "103  entertainment\n",
      "104           tech\n",
      "105           tech\n",
      "\n",
      "[106 rows x 1 columns]\n",
      "\n",
      " Logistic Regression Prediction Probabilities\n",
      "     Entertainment      Tech\n",
      "0         0.658963  0.341037\n",
      "1         0.112167  0.887833\n",
      "2         0.154608  0.845392\n",
      "3         0.941835  0.058165\n",
      "4         0.943972  0.056028\n",
      "..             ...       ...\n",
      "101       0.597341  0.402659\n",
      "102       0.792064  0.207936\n",
      "103       0.816202  0.183798\n",
      "104       0.137007  0.862993\n",
      "105       0.157883  0.842117\n",
      "\n",
      "[106 rows x 2 columns]\n",
      "\n",
      " Logistic Regression Accuracy\n",
      "0.9905660377358491\n"
     ]
    }
   ],
   "source": [
    "# Matthew Young\n",
    "# Logistic Regression Classifier\n",
    "\n",
    "# Import Libraries for LR, metrics, and graph plotting\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import log_loss\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "traindf = df\n",
    "text_transformer = TfidfVectorizer()\n",
    "X_train_text = text_transformer.fit_transform(traindf['Text'])\n",
    "X_test_text = text_transformer.transform(testdf['Text'])\n",
    "\n",
    "y = traindf['Category']  #Target Variable of Category being Tech or Entertainment\n",
    "\n",
    "## L2 Regularization LR Classifer\n",
    "logreg = LogisticRegression(penalty='l2', solver='lbfgs', C=1, multi_class='multinomial')\n",
    "logreg.fit(X_train_text, y)\n",
    "\n",
    "logreg_prediction = logreg.predict(X_test_text)\n",
    "logreg_prediction_prob = logreg.predict_proba(X_test_text)\n",
    "\n",
    "# Logistic Regression Summary\n",
    "logreg_predict_df = pd.DataFrame(data=logreg_prediction, columns=['Prediction'])\n",
    "logreg_prediction_prob_df = pd.DataFrame(data=logreg_prediction_prob, columns=['Entertainment', 'Tech'])\n",
    "logreg_score = logreg.score(X_test_text, testdf['Category'])\n",
    "\n",
    "print('\\n', \"Logistic Regression Predictions\")\n",
    "print(logreg_predict_df)\n",
    "print('\\n', \"Logistic Regression Prediction Probabilities\")\n",
    "print(logreg_prediction_prob_df)\n",
    "print('\\n', \"Logistic Regression Accuracy\")\n",
    "print(logreg_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LR Plot\n",
    "# X-Axis: Lambda\n",
    "# Y-Axis: Training/Log Loss\n",
    "\n",
    "lambda_values = [0.00000001, 0.0000001, 0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1]\n",
    "training_loss = []\n",
    "for _lambda in lambda_values:\n",
    "    logreg = LogisticRegression(penalty='l2', solver='lbfgs', C=_lambda, multi_class='multinomial')\n",
    "    logreg.fit(X_train_text, y)\n",
    "    training_loss.append(logreg.score(X_train_text, y))\n",
    "\n",
    "plt.plot(lambda_values, training_loss)\n",
    "plt.xlabel('Œª Values')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization is the technique used to reduce error by fitting a function appropriately on the given training set and avoid under/overfitting.\n",
    "\n",
    "The effect of the regularization parameter Œª on the outcome in terms of bias and variance is that as the lambda parameter increases, training error increases. Regularization forces parameters to be close to 0 which causes the variance to decrease, but as we are allowing less flexibility, the model moves away from the true values, thus increasing bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "###### Train a Naive Bayes classifier using all articles features. Report the (i) top-20 most identifiable words that are most likely to occur in the articles over two classes using your NB classifier, and (ii) the top-20 words that maximize the following quantity ùëÉ(ùëãùë§=1|ùëå=ùë¶)/ùëÉ(ùëãùë§=1|ùëå‚â†ùë¶). Which list of words describe the two classes better? Briefly explain your reasoning. - She's going to change some stuff and make an announcement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9953271028037384 or 0.9953271028037384\n"
     ]
    }
   ],
   "source": [
    "# Emily\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "# X_train is vectorised features, Y_train is the classes of each row / target variable\n",
    "X_train = vector2.toarray()\n",
    "target_col = df[\"Category\"]\n",
    "Y_train = target_col.to_numpy()\n",
    "\n",
    "NB_clf = MultinomialNB()\n",
    "NB_clf.fit(X_train, Y_train)\n",
    "\n",
    "# Training accuracy\n",
    "y_train_pred = NB_clf.predict(X_train)\n",
    "train_acc = metrics.accuracy_score(Y_train, y_train_pred)\n",
    "# or\n",
    "train_acc_2 = NB_clf.score(X_train, Y_train)\n",
    "\n",
    "print(train_acc, \"or\", train_acc_2)\n",
    "\n",
    "# Testing accuracy\n",
    "#Y_pred = NB_clf.predict(X_test)\n",
    "#print(\"Test Accuracy:\",metrics.accuracy_score(Y_Test, Y_pred))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Tech:\n",
      "said          892\n",
      "people        507\n",
      "new           304\n",
      "mobile        290\n",
      "mr            288\n",
      "one           286\n",
      "also          273\n",
      "would         267\n",
      "could         255\n",
      "technology    247\n",
      "use           228\n",
      "users         214\n",
      "net           214\n",
      "software      213\n",
      "games         212\n",
      "us            210\n",
      "music         203\n",
      "many          202\n",
      "year          201\n",
      "phone         196\n",
      "dtype: int64\n",
      "Top 20 Entertainment:\n",
      "said      465\n",
      "film      420\n",
      "best      324\n",
      "year      241\n",
      "music     210\n",
      "also      206\n",
      "us        201\n",
      "new       196\n",
      "one       193\n",
      "show      180\n",
      "first     155\n",
      "awards    137\n",
      "tv        130\n",
      "last      127\n",
      "uk        127\n",
      "actor     126\n",
      "number    124\n",
      "band      123\n",
      "mr        120\n",
      "star      118\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Df for grouping - vectorsied df with target df \n",
    "count_df = pd.concat([v1_df_emily,target_col], axis=1)\n",
    "\n",
    "# Grouped data by category\n",
    "grouped = count_df.groupby(count_df.Category)\n",
    "\n",
    "# Split data into df of each category\n",
    "# dropped category column for sum\n",
    "tech_df = grouped.get_group(\"tech\")\n",
    "tech_df = tech_df.drop(\"Category\", axis=1)\n",
    "\n",
    "ent_df = grouped.get_group(\"entertainment\")\n",
    "ent_df = ent_df.drop(\"Category\", axis=1)\n",
    "\n",
    "# Summed each colum in each df\n",
    "sum_tech_series = tech_df.sum()\n",
    "sum_ent_series = ent_df.sum()\n",
    "\n",
    "# Sorting each series to get top 20\n",
    "sorted_tech = sum_tech_series.sort_values(ascending = False)\n",
    "top_20_tech = sorted_tech[:20]\n",
    "print(\"Top 20 Tech:\")\n",
    "print(top_20_tech)\n",
    "\n",
    "sorted_ent = sum_ent_series.sort_values(ascending = False)\n",
    "top_20_ent = sorted_ent[:20]\n",
    "print(\"Top 20 Entertainment:\")\n",
    "print(top_20_ent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Tech:\n",
      "people        1.341420\n",
      "said          1.223264\n",
      "mobile        1.168540\n",
      "technology    1.064229\n",
      "use           0.997203\n",
      "users         0.995370\n",
      "software      0.993056\n",
      "net           0.990697\n",
      "games         0.981481\n",
      "phone         0.944791\n",
      "could         0.941503\n",
      "mr            0.909314\n",
      "computer      0.894544\n",
      "would         0.892760\n",
      "digital       0.871711\n",
      "microsoft     0.856481\n",
      "many          0.852930\n",
      "service       0.843495\n",
      "online        0.838679\n",
      "internet      0.833805\n",
      "dtype: float64\n",
      "Top 20 Entertainment:\n",
      "film        1.358491\n",
      "best        1.164420\n",
      "awards      0.793716\n",
      "actor       0.791672\n",
      "band        0.788270\n",
      "show        0.763664\n",
      "star        0.758976\n",
      "award       0.756608\n",
      "album       0.729444\n",
      "year        0.729142\n",
      "actress     0.712264\n",
      "singer      0.709906\n",
      "chart       0.708266\n",
      "oscar       0.705189\n",
      "british     0.695703\n",
      "director    0.694975\n",
      "films       0.684314\n",
      "music       0.677106\n",
      "stars       0.676887\n",
      "musical     0.666725\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "total_tech = tech_df.shape[0]\n",
    "prob_word_given_tech = sum_tech_series / total_tech\n",
    "\n",
    "total_ent = ent_df.shape[0]\n",
    "prob_word_given_ent = sum_ent_series / total_ent\n",
    "\n",
    "# prob of word in tech / prob of word not in tech - with smoothing\n",
    "tech_series = (prob_word_given_tech + 1) / (prob_word_given_ent + 2)\n",
    "\n",
    "# prob of word in entertainment / prob of word not in entertainment - with smoothing\n",
    "ent_series = (prob_word_given_ent + 1) / (prob_word_given_tech + 2)\n",
    "\n",
    "# Sorting each series to get top 20\n",
    "sorted_tech2 = tech_series.sort_values(ascending = False)\n",
    "top_20_tech2 = sorted_tech2[:20]\n",
    "print(\"Top 20 Tech:\")\n",
    "print(top_20_tech2)\n",
    "\n",
    "sorted_ent2 = ent_series.sort_values(ascending = False)\n",
    "top_20_ent2 = sorted_ent2[:20]\n",
    "print(\"Top 20 Entertainment:\")\n",
    "print(top_20_ent2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference - there is a difference just by looking at them, the top20 words from (ii) look to be more relevent than the top20 words from (i). So (ii) seems to be the better formula.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NTS - What I need to do\n",
    "Cut the data df into the two classes\n",
    "add up word count for each column \n",
    "pick ones with the top counts\n",
    "\n",
    "count of the class i'm looking at / count of class i'm not looking at\n",
    "\n",
    "df.sum() - give array \n",
    "arc sort "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft Value Margin (SVM)\n",
    "###### Train your SVM classification models on the training dataset. You need to report two surface plots for: (i) the soft-margin linear SVM with your choice of misclassification penalty (ùê∂), and (ii) the hard-margin RBF kernel with your choice of kernel width (œÉ). Explain the impact of penalty ùê∂ on the soft-margin decision boundaries, as well as the kernel hyperparameter on the hard-margin decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Humza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest Neighbor\n",
    "###### Consider the neural network with the following hyperparameters: the initial weights uniformly drawn in range [0,0.1] with learning rate 0.01.\n",
    "######  ‚óè Train a single hidden layer neural network using the hyperparameters on the training dataset, except for the number of hidden units (x) which should vary among 5, 20, and 40. Run the optimization for 100 epochs each time. Namely, the input layer consists of n features x = [x1, ..., xn]T , the hidden layer has x nodes z = [z1, ..., zx]T , and the output layer is a probability distribution y = [y1, y2]T over two classes.\n",
    "######  ‚óè Plot the average training cross-entropy loss as shown below on the y-axis versus the number of hidden units on the x-axis. Explain the effect of numbers of hidden units. ùê∂ùëüùëúùë†ùë†ùê∏ùëõùë°ùëüùëúùëùùë¶ùêøùëúùë†ùë† =‚àí ùëñ=1 2 Œ£ ùë¶ùëñ log(ùë¶ùëñ ^ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8471e12cab07c52a942a04ee5723bc328a80c174fa96b210ede9dde21e6dc81a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
