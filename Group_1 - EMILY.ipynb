{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BBC News Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Exploratory Data Analytics\n",
    "###### (a) Load the dataset and construct a feature vector for each article in the. You need to report the number of articles, and the number of extracted features. Show 5 example articles with their extracted features using a dataframe.\n",
    "###### (b) Conduct term frequency analysis and report three plots: (i) top-50 term frequency distribution across the entire dataset, (ii) term frequency distribution for respective class of articles, and (iii) class distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleId</th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1821</td>\n",
       "      <td>johnny denise lose passport johnny vaughan den...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>522</td>\n",
       "      <td>bt offers free net phone calls bt offering cus...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>636</td>\n",
       "      <td>power people says hp digital revolution focuse...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>170</td>\n",
       "      <td>stars gear bafta ceremony film stars across gl...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>85</td>\n",
       "      <td>controversial film tops festival controversial...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ArticleId                                               Text       Category\n",
       "0      1821  johnny denise lose passport johnny vaughan den...  entertainment\n",
       "1       522  bt offers free net phone calls bt offering cus...           tech\n",
       "2       636  power people says hp digital revolution focuse...           tech\n",
       "3       170  stars gear bafta ceremony film stars across gl...  entertainment\n",
       "4        85  controversial film tops festival controversial...  entertainment"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"train.csv\", skiprows=0, header=0, na_values= \"\", dtype=str)\n",
    "df.head()\n",
    "\n",
    "## Load Test Data ##\n",
    "testdf = pd.read_csv(\"test.csv\", skiprows=0, header=0, na_values= \"\", dtype=str)\n",
    "testdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 1\n",
      "article vector\n",
      " [[0 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "\n",
      "article vector (5 articles)\n",
      " [[0 0 0 ... 2 0 0]\n",
      " [1 1 1 ... 3 2 1]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 1 0 0]\n",
      " [1 0 0 ... 0 0 0]]\n",
      "\n",
      " Method 2\n",
      "article vector\n",
      " [[0.         0.02011467 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "\n",
      "article vector (5 articles)\n",
      " [[0.         0.         0.         ... 0.08538436 0.         0.        ]\n",
      " [0.04462451 0.05531093 0.05531093 ... 0.11112713 0.11062186 0.05531093]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.04329183 0.         0.        ]\n",
      " [0.05018108 0.         0.         ... 0.         0.         0.        ]]\n",
      "\n",
      "Articles: 428 , Extracted Features: 13518\n"
     ]
    }
   ],
   "source": [
    "articles_text = df[\"Text\"].to_numpy()\n",
    "\n",
    "#select 5 random articles for task 1\n",
    "random_sample = random.sample(list(articles_text), 5)\n",
    "\n",
    "## APPROACH ONE ##\n",
    "vectorizer1 = CountVectorizer()\n",
    "vectorizer1.fit(articles_text)\n",
    "\n",
    "vectorizer1_sample = CountVectorizer()\n",
    "vectorizer1_sample.fit(random_sample)\n",
    "\n",
    "#Summary\n",
    "#print(f'vector vocabulary - {vectorizer.vocabulary_}\\n')\n",
    "\n",
    "# encode document\n",
    "vector1 = vectorizer1.transform(articles_text)\n",
    "vector1_sample = vectorizer1_sample.transform(random_sample)\n",
    "\n",
    "# summarize encoded vector\n",
    "print(\"Method 1\")\n",
    "print(f'article vector\\n {vector1.toarray()}')\n",
    "print(f'\\narticle vector (5 articles)\\n {vector1_sample.toarray()}')\n",
    "\n",
    "## APPROACH TWO ##\n",
    "vectorizer2 = TfidfVectorizer()\n",
    "vectorizer2.fit(articles_text)\n",
    "\n",
    "vectorizer2_sample = TfidfVectorizer()\n",
    "vectorizer2_sample.fit(random_sample)\n",
    "\n",
    "#Summary\n",
    "#print(f'vector vocabulary - {vectorizer2.vocabulary_}\\n')\n",
    "\n",
    "# encode document\n",
    "vector2 = vectorizer2.fit_transform(articles_text)\n",
    "vector2_sample = vectorizer2_sample.transform(random_sample)\n",
    "\n",
    "# summarize encoded vector\n",
    "print('\\n', \"Method 2\")\n",
    "print(f'article vector\\n {vector2.toarray()}')\n",
    "print(f'\\narticle vector (5 articles)\\n {vector2_sample.toarray()}')\n",
    "print('\\nArticles:', vector2.shape[0], ', Extracted Features:', vector2.shape[1])\n",
    "\n",
    "# for my task\n",
    "## APPROACH ONE ##\n",
    "vectorizer1 = CountVectorizer()\n",
    "vectorizer1.fit(articles_text)\n",
    "\n",
    "vectorizer1_sample = CountVectorizer()\n",
    "vectorizer1_sample.fit(random_sample)\n",
    "\n",
    "# encode document\n",
    "vector1 = vectorizer1.transform(articles_text)\n",
    "vector1 = vector1.toarray()\n",
    "v1_names = vectorizer1.get_feature_names_out()\n",
    "\n",
    "vector1_sample = vectorizer1_sample.transform(random_sample)\n",
    "vector1_sample = vector1_sample.toarray()\n",
    "v1_sample_names = vectorizer1_sample.get_feature_names_out()\n",
    "\n",
    "#map sample to feature names\n",
    "v1_sample_df = pd.DataFrame(data=zip(v1_sample_names, vector1_sample[0], vector1_sample[1], vector1_sample[2], vector1_sample[3], vector1_sample[4], sum([vector1_sample[0], vector1_sample[1], vector1_sample[2], vector1_sample[3], vector1_sample[4]])), columns=['Features', 'Article 1', 'Article 2', 'Article 3', 'Article 4', 'Article 5', 'Total'])\n",
    "\n",
    "v1_df_emily = pd.DataFrame(data=vector1, columns = v1_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Classification Models Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "###### Train your logistic regression classifier with L2-regularization. Consider different values of the regularization term Œª. Describe the effect of the regularization parameter Œª on the outcome in terms of bias and variance. Report the plot generated for specific Œª values with training loss on the y-axis versus Œª on the x-axis to support your claim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Logistic Regression Predictions\n",
      "        Prediction\n",
      "0    entertainment\n",
      "1             tech\n",
      "2             tech\n",
      "3    entertainment\n",
      "4    entertainment\n",
      "..             ...\n",
      "101  entertainment\n",
      "102  entertainment\n",
      "103  entertainment\n",
      "104           tech\n",
      "105           tech\n",
      "\n",
      "[106 rows x 1 columns]\n",
      "\n",
      " Logistic Regression Prediction Probabilities\n",
      "     Entertainment      Tech\n",
      "0         0.658963  0.341037\n",
      "1         0.112167  0.887833\n",
      "2         0.154608  0.845392\n",
      "3         0.941835  0.058165\n",
      "4         0.943972  0.056028\n",
      "..             ...       ...\n",
      "101       0.597341  0.402659\n",
      "102       0.792064  0.207936\n",
      "103       0.816202  0.183798\n",
      "104       0.137007  0.862993\n",
      "105       0.157883  0.842117\n",
      "\n",
      "[106 rows x 2 columns]\n",
      "\n",
      " Logistic Regression Accuracy\n",
      "0.9905660377358491\n"
     ]
    }
   ],
   "source": [
    "# Matthew Young\n",
    "# Logistic Regression Classifier\n",
    "\n",
    "# Import Libraries for LR, metrics, and graph plotting\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import log_loss\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "traindf = df\n",
    "text_transformer = TfidfVectorizer()\n",
    "X_train_text = text_transformer.fit_transform(traindf['Text'])\n",
    "X_test_text = text_transformer.transform(testdf['Text'])\n",
    "\n",
    "y = traindf['Category']  #Target Variable of Category being Tech or Entertainment\n",
    "\n",
    "## L2 Regularization LR Classifer\n",
    "logreg = LogisticRegression(penalty='l2', solver='lbfgs', C=1, multi_class='multinomial')\n",
    "logreg.fit(X_train_text, y)\n",
    "\n",
    "logreg_prediction = logreg.predict(X_test_text)\n",
    "logreg_prediction_prob = logreg.predict_proba(X_test_text)\n",
    "\n",
    "# Logistic Regression Summary\n",
    "logreg_predict_df = pd.DataFrame(data=logreg_prediction, columns=['Prediction'])\n",
    "logreg_prediction_prob_df = pd.DataFrame(data=logreg_prediction_prob, columns=['Entertainment', 'Tech'])\n",
    "logreg_score = logreg.score(X_test_text, testdf['Category'])\n",
    "\n",
    "print('\\n', \"Logistic Regression Predictions\")\n",
    "print(logreg_predict_df)\n",
    "print('\\n', \"Logistic Regression Prediction Probabilities\")\n",
    "print(logreg_prediction_prob_df)\n",
    "print('\\n', \"Logistic Regression Accuracy\")\n",
    "print(logreg_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LR Plot\n",
    "# X-Axis: Lambda\n",
    "# Y-Axis: Training/Log Loss\n",
    "\n",
    "lambda_values = [0.00000001, 0.0000001, 0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1]\n",
    "training_loss = []\n",
    "for _lambda in lambda_values:\n",
    "    logreg = LogisticRegression(penalty='l2', solver='lbfgs', C=_lambda, multi_class='multinomial')\n",
    "    logreg.fit(X_train_text, y)\n",
    "    training_loss.append(log_loss(y, logreg.predict_proba(X_train_text)))\n",
    "\n",
    "plt.plot(lambda_values, training_loss)\n",
    "plt.xlabel('Œª Values')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization is the technique used to reduce error by fitting a function appropriately on the given training set and avoid under/overfitting.\n",
    "\n",
    "The effect of the regularization parameter Œª on the outcome in terms of bias and variance is that as the lambda parameter increases, training error increases. Regularization forces parameters to be close to 0 which causes the variance to decrease, but as we are allowing less flexibility, the model moves away from the true values, thus increasing bias.\n",
    "\n",
    "The plot shows the inverse of this as the C parameter in the LogisticRegression class is the inverse of regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "###### Train a Naive Bayes classifier using all articles features. Report the (i) top-20 most identifiable words that are most likely to occur in the articles over two classes using your NB classifier, and (ii) the top-20 words that maximize the following quantity ùëÉ(ùëãùë§=1|ùëå=ùë¶)/ùëÉ(ùëãùë§=1|ùëå‚â†ùë¶). Which list of words describe the two classes better? Briefly explain your reasoning. - She's going to change some stuff and make an announcement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9953271028037384 or 0.9953271028037384\n"
     ]
    }
   ],
   "source": [
    "# Emily\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "# X_train is vectorised features, Y_train is the classes of each row / target variable\n",
    "X_train = vector2.toarray()\n",
    "target_col = df[\"Category\"]\n",
    "Y_train = target_col.to_numpy()\n",
    "\n",
    "NB_clf = MultinomialNB()\n",
    "NB_clf.fit(X_train, Y_train)\n",
    "\n",
    "# Training accuracy\n",
    "y_train_pred = NB_clf.predict(X_train)\n",
    "train_acc = metrics.accuracy_score(Y_train, y_train_pred)\n",
    "# or\n",
    "train_acc_2 = NB_clf.score(X_train, Y_train)\n",
    "\n",
    "print(train_acc, \"or\", train_acc_2)\n",
    "\n",
    "# Testing accuracy\n",
    "#Y_pred = NB_clf.predict(X_test)\n",
    "#print(\"Test Accuracy:\",metrics.accuracy_score(Y_Test, Y_pred))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Tech:\n",
      "said          892\n",
      "people        507\n",
      "new           304\n",
      "mobile        290\n",
      "mr            288\n",
      "one           286\n",
      "also          273\n",
      "would         267\n",
      "could         255\n",
      "technology    247\n",
      "use           228\n",
      "users         214\n",
      "net           214\n",
      "software      213\n",
      "games         212\n",
      "us            210\n",
      "music         203\n",
      "many          202\n",
      "year          201\n",
      "phone         196\n",
      "dtype: int64\n",
      "Top 20 Entertainment:\n",
      "said      465\n",
      "film      420\n",
      "best      324\n",
      "year      241\n",
      "music     210\n",
      "also      206\n",
      "us        201\n",
      "new       196\n",
      "one       193\n",
      "show      180\n",
      "first     155\n",
      "awards    137\n",
      "tv        130\n",
      "last      127\n",
      "uk        127\n",
      "actor     126\n",
      "number    124\n",
      "band      123\n",
      "mr        120\n",
      "star      118\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Df for grouping - vectorsied df with target df \n",
    "count_df = pd.concat([v1_df_emily,target_col], axis=1)\n",
    "\n",
    "# Grouped data by category\n",
    "grouped = count_df.groupby(count_df.Category)\n",
    "\n",
    "# Split data into df of each category\n",
    "# dropped category column for sum\n",
    "tech_df = grouped.get_group(\"tech\")\n",
    "tech_df = tech_df.drop(\"Category\", axis=1)\n",
    "\n",
    "ent_df = grouped.get_group(\"entertainment\")\n",
    "ent_df = ent_df.drop(\"Category\", axis=1)\n",
    "\n",
    "# Summed each colum in each df\n",
    "sum_tech_series = tech_df.sum()\n",
    "sum_ent_series = ent_df.sum()\n",
    "\n",
    "# Sorting each series to get top 20\n",
    "sorted_tech = sum_tech_series.sort_values(ascending = False)\n",
    "top_20_tech = sorted_tech[:20]\n",
    "print(\"Top 20 Tech:\")\n",
    "print(top_20_tech)\n",
    "\n",
    "sorted_ent = sum_ent_series.sort_values(ascending = False)\n",
    "top_20_ent = sorted_ent[:20]\n",
    "print(\"Top 20 Entertainment:\")\n",
    "print(top_20_ent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Tech:\n",
      "users         107.500000\n",
      "software      107.000000\n",
      "mobile         97.000000\n",
      "microsoft      77.500000\n",
      "broadband      64.500000\n",
      "virus          61.500000\n",
      "firms          57.000000\n",
      "pc             54.500000\n",
      "net            53.750000\n",
      "technology     49.600000\n",
      "phones         48.333333\n",
      "spam           42.500000\n",
      "gadget         36.000000\n",
      "games          35.500000\n",
      "consumer       34.500000\n",
      "mobiles        34.000000\n",
      "gadgets        33.500000\n",
      "windows        33.500000\n",
      "machines       33.500000\n",
      "phone          32.833333\n",
      "dtype: float64\n",
      "Top 20 Entertainment:\n",
      "actress        45.500000\n",
      "singer         45.000000\n",
      "oscar          44.000000\n",
      "band           41.333333\n",
      "stars          38.000000\n",
      "album          33.000000\n",
      "aviator        31.500000\n",
      "chart          30.000000\n",
      "nominated      27.500000\n",
      "rock           26.500000\n",
      "festival       26.500000\n",
      "actor          25.400000\n",
      "nominations    24.000000\n",
      "charles        23.500000\n",
      "foxx           22.000000\n",
      "comedy         21.666667\n",
      "oscars         21.500000\n",
      "starring       21.000000\n",
      "singles        19.000000\n",
      "musical        18.250000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Will break since some words are in an article multiple times (prob can't be more than one)\n",
    "#total_tech = tech_df.shape[0]\n",
    "#prob_word_given_tech = sum_tech_series / total_tech\n",
    "#total_ent = ent_df.shape[0]\n",
    "#prob_word_given_ent = sum_ent_series / total_ent\n",
    "\n",
    "# word in tech / word not in tech - with smoothing\n",
    "tech_series = (sum_tech_series + 1) / (sum_ent_series + 2)\n",
    "\n",
    "# word in entertainment / word not in entertainment - with smoothing\n",
    "ent_series = (sum_ent_series + 1) / (sum_tech_series + 2)\n",
    "\n",
    "# Sorting each series to get top 20\n",
    "sorted_tech2 = tech_series.sort_values(ascending = False)\n",
    "top_20_tech2 = sorted_tech2[:20]\n",
    "print(\"Top 20 Tech:\")\n",
    "print(top_20_tech2)\n",
    "\n",
    "sorted_ent2 = ent_series.sort_values(ascending = False)\n",
    "top_20_ent2 = sorted_ent2[:20]\n",
    "print(\"Top 20 Entertainment:\")\n",
    "print(top_20_ent2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference - there is a difference just by looking at them, the top20 words from (ii) look to be more relevent than the top20 words from (i). So (ii) seems to be the better formula.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NTS - What I need to do\n",
    "Cut the data df into the two classes\n",
    "add up word count for each column \n",
    "pick ones with the top counts\n",
    "\n",
    "count of the class i'm looking at / count of class i'm not looking at\n",
    "\n",
    "df.sum() - give array \n",
    "arc sort "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft Value Margin (SVM)\n",
    "###### Train your SVM classification models on the training dataset. You need to report two surface plots for: (i) the soft-margin linear SVM with your choice of misclassification penalty (ùê∂), and (ii) the hard-margin RBF kernel with your choice of kernel width (œÉ). Explain the impact of penalty ùê∂ on the soft-margin decision boundaries, as well as the kernel hyperparameter on the hard-margin decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Humza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest Neighbor\n",
    "###### Consider the neural network with the following hyperparameters: the initial weights uniformly drawn in range [0,0.1] with learning rate 0.01.\n",
    "######  ‚óè Train a single hidden layer neural network using the hyperparameters on the training dataset, except for the number of hidden units (x) which should vary among 5, 20, and 40. Run the optimization for 100 epochs each time. Namely, the input layer consists of n features x = [x1, ..., xn]T , the hidden layer has x nodes z = [z1, ..., zx]T , and the output layer is a probability distribution y = [y1, y2]T over two classes.\n",
    "######  ‚óè Plot the average training cross-entropy loss as shown below on the y-axis versus the number of hidden units on the x-axis. Explain the effect of numbers of hidden units. ùê∂ùëüùëúùë†ùë†ùê∏ùëõùë°ùëüùëúùëùùë¶ùêøùëúùë†ùë† =‚àí ùëñ=1 2 Œ£ ùë¶ùëñ log(ùë¶ùëñ ^ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = pd.read_csv(\"train.csv\", skiprows=0, header=0, na_values= \"\", dtype=str)\n",
    "testDF = pd.read_csv(\"test.csv\", skiprows=0, header=0, na_values= \"\", dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 1\n",
      "article vector\n",
      " [[0 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "\n",
      "article vector (5 articles)\n",
      " [[1 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 1 1 0]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [2 1 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "\n",
      " Method 2\n",
      "article vector\n",
      " [[0.         0.02011467 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "\n",
      "article vector (5 articles)\n",
      " [[0.04668298 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.05390407 0.05390407 0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.04161495]\n",
      " [0.06021438 0.03731709 0.03731709 ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "\n",
      "Articles: 428 , Extracted Features: 13518\n"
     ]
    }
   ],
   "source": [
    "articles_text = trainDF[\"Text\"].to_numpy()\n",
    "\n",
    "#select 5 random articles for task 1\n",
    "random_sample = random.sample(list(articles_text), 5)\n",
    "\n",
    "## APPROACH ONE ##\n",
    "vectorizer1 = CountVectorizer()\n",
    "vectorizer1.fit(articles_text)\n",
    "\n",
    "vectorizer1_sample = CountVectorizer()\n",
    "vectorizer1_sample.fit(random_sample)\n",
    "\n",
    "#Summary\n",
    "#print(f'vector vocabulary - {vectorizer.vocabulary_}\\n')\n",
    "\n",
    "# encode document\n",
    "vector1 = vectorizer1.transform(articles_text)\n",
    "vector1_sample = vectorizer1_sample.transform(random_sample)\n",
    "\n",
    "# summarize encoded vector\n",
    "print(\"Method 1\")\n",
    "print(f'article vector\\n {vector1.toarray()}')\n",
    "print(f'\\narticle vector (5 articles)\\n {vector1_sample.toarray()}')\n",
    "\n",
    "## APPROACH TWO ##\n",
    "vectorizer2 = TfidfVectorizer()\n",
    "vectorizer2.fit(articles_text)\n",
    "\n",
    "vectorizer2_sample = TfidfVectorizer()\n",
    "vectorizer2_sample.fit(random_sample)\n",
    "\n",
    "#Summary\n",
    "#print(f'vector vocabulary - {vectorizer.vocabulary_}\\n')\n",
    "\n",
    "# encode document\n",
    "vector2 = vectorizer2.transform(articles_text)\n",
    "vector2_sample = vectorizer2_sample.transform(random_sample)\n",
    "\n",
    "# summarize encoded vector\n",
    "print('\\n', \"Method 2\")\n",
    "print(f'article vector\\n {vector2.toarray()}')\n",
    "print(f'\\narticle vector (5 articles)\\n {vector2_sample.toarray()}')\n",
    "print('\\nArticles:', vector2.shape[0], ', Extracted Features:', vector2.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42, 13518) (106, 13518) (42,) (106,)\n",
      "(128, 13518) (106, 13518) (128,) (106,)\n",
      "(214, 13518) (106, 13518) (214,) (106,)\n",
      "(299, 13518) (106, 13518) (299,) (106,)\n",
      "(385, 13518) (106, 13518) (385,) (106,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.1_train</th>\n",
       "      <th>0.3_train</th>\n",
       "      <th>0.5_train</th>\n",
       "      <th>0.7_train</th>\n",
       "      <th>0.9_train</th>\n",
       "      <th>0.1_test</th>\n",
       "      <th>0.3_test</th>\n",
       "      <th>0.5_test</th>\n",
       "      <th>0.7_test</th>\n",
       "      <th>0.9_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LR</th>\n",
       "      <td>0.617949</td>\n",
       "      <td>0.465043</td>\n",
       "      <td>0.348566</td>\n",
       "      <td>0.267723</td>\n",
       "      <td>0.213213</td>\n",
       "      <td>0.717488</td>\n",
       "      <td>0.52406</td>\n",
       "      <td>0.403188</td>\n",
       "      <td>0.31008</td>\n",
       "      <td>0.243248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NB</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.995434</td>\n",
       "      <td>0.996785</td>\n",
       "      <td>0.994819</td>\n",
       "      <td>0.884259</td>\n",
       "      <td>0.984026</td>\n",
       "      <td>0.985915</td>\n",
       "      <td>0.991736</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NN</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0.1_train 0.3_train 0.5_train 0.7_train 0.9_train  0.1_test  0.3_test  \\\n",
       "LR   0.617949  0.465043  0.348566  0.267723  0.213213  0.717488   0.52406   \n",
       "NB        1.0       1.0  0.995434  0.996785  0.994819  0.884259  0.984026   \n",
       "SVM         0         0         0         0         0         0         0   \n",
       "NN          0         0         0         0         0         0         0   \n",
       "\n",
       "     0.5_test  0.7_test  0.9_test  \n",
       "LR   0.403188   0.31008  0.243248  \n",
       "NB   0.985915  0.991736       1.0  \n",
       "SVM         0         0         0  \n",
       "NN          0         0         0  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import floor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "m = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "training_output = pd.DataFrame(columns=m)\n",
    "testing_output = pd.DataFrame(columns=m)\n",
    "\n",
    "NUM_ARTICLES = vector1.shape[0]\n",
    "\n",
    "# Transform the data into a format that can be used by the classifier\n",
    "text_transformer = TfidfVectorizer()\n",
    "X_train_text = text_transformer.fit_transform(trainDF['Text'])\n",
    "X_test_text = text_transformer.transform(testDF['Text'])\n",
    "\n",
    "# LOGISTIC REGRESSION\n",
    "training_accuracies = {}\n",
    "testing_accuracies = {}\n",
    "for m_value in m:\n",
    "    TRAIN_LENGTH = floor(m_value * NUM_ARTICLES)\n",
    "    # Test how differing sizes of training set data affect test/train accuracy\n",
    "    X_train = X_train_text[0:TRAIN_LENGTH]  # First TRAIN_LENGTH articles\n",
    "    Y_train = trainDF['Category'][0:TRAIN_LENGTH] # First TRAIN_LENGTH categories\n",
    "\n",
    "    # Test Data Unchanged\n",
    "    X_test = X_test_text    # All test data text\n",
    "    Y_test = testDF['Category'] # All test data categories\n",
    "\n",
    "    print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape) # Testing purposes\n",
    "\n",
    "    # Train & Fit LR Model\n",
    "    logreg = LogisticRegression(penalty='l2', solver='lbfgs', C=m_value, multi_class='multinomial')\n",
    "    logreg.fit(X_train, Y_train)\n",
    "\n",
    "    training_loss = log_loss(Y_train, logreg.predict_proba(X_train))\n",
    "    testing_loss = log_loss(Y_test, logreg.predict_proba(X_test))\n",
    "\n",
    "    # LR Model Accuracies\n",
    "    training_accuracies[m_value] = training_loss\n",
    "    testing_accuracies[m_value] = testing_loss\n",
    "\n",
    "training_output = pd.concat(objs=[training_output, pd.DataFrame(training_accuracies, index=['LR'])])\n",
    "testing_output = pd.concat(objs=[testing_output, pd.DataFrame(testing_accuracies, index=['LR'])])\n",
    "\n",
    "# NAIVE BAYES\n",
    "training_accuracies = {}\n",
    "testing_accuracies = {}\n",
    "for m_value in m:\n",
    "    TRAIN_LENGTH = floor(m_value * NUM_ARTICLES)\n",
    "    VECTOR_ARRAY = vector1.toarray() # change this to vector 2 if needed\n",
    "    X_train = VECTOR_ARRAY[0:TRAIN_LENGTH]\n",
    "    X_test = VECTOR_ARRAY[TRAIN_LENGTH:]\n",
    "    Y_train = trainDF[\"Category\"].to_list()[:TRAIN_LENGTH]\n",
    "    Y_test = trainDF[\"Category\"].to_list()[TRAIN_LENGTH:]\n",
    "\n",
    "    # train your model here\n",
    "    NB_clf = MultinomialNB()\n",
    "    NB_clf.fit(X_train, Y_train)\n",
    "\n",
    "    # put your accuracy calc here\n",
    "    Y_train_pred = NB_clf.predict(X_train)\n",
    "    Y_pred = NB_clf.predict(X_test)\n",
    "    training_accuracies[m_value] = metrics.f1_score(Y_train, Y_train_pred, pos_label = \"tech\")\n",
    "    testing_accuracies[m_value] = metrics.f1_score(Y_test, Y_pred, pos_label = \"tech\")\n",
    "    \n",
    "training_output = pd.concat(objs=[training_output, pd.DataFrame(training_accuracies, index=['NB'])])\n",
    "testing_output = pd.concat(objs=[testing_output, pd.DataFrame(testing_accuracies, index=['NB'])])\n",
    "\n",
    "# SVM\n",
    "training_accuracies = {}\n",
    "testing_accuracies = {}\n",
    "for m_value in m:\n",
    "    TRAIN_LENGTH = floor(m_value * NUM_ARTICLES)\n",
    "    VECTOR_ARRAY = vector1.toarray() # change this to vector 2 if needed\n",
    "    X_train = VECTOR_ARRAY[0:TRAIN_LENGTH]\n",
    "    X_test = VECTOR_ARRAY[TRAIN_LENGTH:]\n",
    "    Y_train = trainDF[\"Category\"].to_list()[:TRAIN_LENGTH]\n",
    "    Y_test = trainDF[\"Category\"].to_list()[TRAIN_LENGTH:]\n",
    "\n",
    "    # train your model here\n",
    "\n",
    "    # put your accuracy calc here\n",
    "    training_accuracies[m_value] = 0\n",
    "    testing_accuracies[m_value] = 0\n",
    "    \n",
    "training_output = pd.concat(objs=[training_output, pd.DataFrame(training_accuracies, index=['SVM'])])\n",
    "testing_output = pd.concat(objs=[testing_output, pd.DataFrame(testing_accuracies, index=['SVM'])])\n",
    "\n",
    "# Not Nearest Neighbour (lol)\n",
    "training_accuracies = {}\n",
    "testing_accuracies = {}\n",
    "for m_value in m:\n",
    "    TRAIN_LENGTH = floor(m_value * NUM_ARTICLES)\n",
    "    VECTOR_ARRAY = vector1.toarray() # change this to vector 2 if needed\n",
    "    X_train = VECTOR_ARRAY[0:TRAIN_LENGTH]\n",
    "    X_test = VECTOR_ARRAY[TRAIN_LENGTH:]\n",
    "    Y_train = trainDF[\"Category\"].to_list()[:TRAIN_LENGTH]\n",
    "    Y_test = trainDF[\"Category\"].to_list()[TRAIN_LENGTH:]\n",
    "\n",
    "    # train your model here\n",
    "\n",
    "    # put your accuracy calc here\n",
    "    training_accuracies[m_value] = 0\n",
    "    testing_accuracies[m_value] = 0\n",
    "    \n",
    "training_output = pd.concat(objs=[training_output, pd.DataFrame(training_accuracies, index=['NN'])])\n",
    "testing_output = pd.concat(objs=[testing_output, pd.DataFrame(testing_accuracies, index=['NN'])])\n",
    "\n",
    "pd.merge(training_output, testing_output, left_index=True, right_index=True, suffixes=('_train', '_test'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b\n",
    "In Q3(b), given models M=[m1, m2, m3] for respective chosen hyper-parameter C = [c1, c2, c3] for instance\n",
    "\n",
    "perform k-fold CV on train.csv to derive 3 training accuracy (TrAcc) and 3 validation accuracy (TsAcc) as follows: TrAcc = [tr1, tr2, tr3], VaAcc = [va1, va2, va3].\n",
    "\n",
    "for each hypter-parameter setting ci, perform classification on test.csv using mi to obtain testing accuracy tsi. \n",
    "\n",
    "Overall, we obtain TsAcc = [ts1, ts2, ts3] across three settings.\n",
    "\n",
    "justify your hyper-parameter setting using TrAcc and VaAcc\n",
    "summarize your observations on overfitting v.s. underfitting across models m1, m2, m3 using TrAcc and TsAcc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: [0.9976642285987076, 0.9964946379554329, 0.9953267522547866] \n",
      "Validation accuracy: [0.9836662106703147, 0.9836662106703147, 0.9836662106703147]\n",
      "Testing accuracy [0.9905660377358491, 0.9716981132075472, 0.9716981132075472]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "X_train = trainDF[\"Text\"]\n",
    "Y_train = trainDF[\"Category\"]\n",
    "X_test = testDF[\"Text\"]\n",
    "Y_test = testDF[\"Category\"]\n",
    "\n",
    "# laplace smoothing hyperparameter\n",
    "# alpha = 0.5\n",
    "NB_m1 = make_pipeline(CountVectorizer(), MultinomialNB(alpha = 0.5))\n",
    "# alpha = 1.0\n",
    "NB_m2 = make_pipeline(CountVectorizer(), MultinomialNB(alpha = 1.0))\n",
    "# alpha = 1.5\n",
    "NB_m3 = make_pipeline(CountVectorizer(), MultinomialNB(alpha = 1.5))\n",
    "\n",
    "cv_results_NB_m1 = cross_validate(NB_m1, X_train, Y_train, return_train_score=True)\n",
    "cv_results_NB_m2 = cross_validate(NB_m2, X_train, Y_train, return_train_score=True)\n",
    "cv_results_NB_m3 = cross_validate(NB_m3, X_train, Y_train, return_train_score=True)\n",
    "\n",
    "NB_TrAcc = [cv_results_NB_m1['train_score'].mean(), cv_results_NB_m2['train_score'].mean(), cv_results_NB_m3['train_score'].mean()]\n",
    "NB_VaAcc = [cv_results_NB_m1['test_score'].mean(), cv_results_NB_m2['test_score'].mean(), cv_results_NB_m3['test_score'].mean()]\n",
    "\n",
    "print(\"Training accuracy:\", NB_TrAcc, \"\\nValidation accuracy:\", NB_VaAcc)\n",
    "\n",
    "\n",
    "\n",
    "NB_m1.fit(X_train, Y_train)\n",
    "Y_pred_m1 = NB_m1.predict(X_test)\n",
    "\n",
    "NB_m2.fit(X_train, Y_train)\n",
    "Y_pred_m2 = NB_m2.predict(X_test)\n",
    "\n",
    "NB_m3.fit(X_train, Y_train)\n",
    "Y_pred_m3 = NB_m3.predict(X_test)\n",
    "\n",
    "NB_TsAcc = [metrics.accuracy_score(Y_test, Y_pred_m1), metrics.accuracy_score(Y_test, Y_pred_m2), metrics.accuracy_score(Y_test, Y_pred_m3)]\n",
    "print(\"Testing accuracy\", NB_TsAcc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9953271028037384\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "NB_m1 = make_pipeline(CountVectorizer(), MultinomialNB(alpha = 1.0))\n",
    "NB_m1.fit(trainDF[\"Text\"], trainDF[\"Category\"])\n",
    "train_acc = NB_m1.score(trainDF[\"Text\"], trainDF[\"Category\"])\n",
    "print(train_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous - Doesn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "test_articles_text = testDF[\"Text\"].to_numpy()\n",
    "\n",
    "test_vectorizer1 = CountVectorizer()\n",
    "test_vectorizer1.fit(test_articles_text)\n",
    "test_vector1 = vectorizer1.transform(articles_text)\n",
    "test_vector1 = test_vector1.toarray()\n",
    "\n",
    "X_train = vector1\n",
    "target_col = trainDF[\"Category\"]\n",
    "Y_train = target_col.to_numpy()\n",
    "\n",
    "X_test = test_vector1\n",
    "Y_test = testDF[\"Category\"].to_numpy()\n",
    "\n",
    "# laplace smoothing hyperparameter\n",
    "# alpha = 0.5\n",
    "NB_m1 = MultinomialNB(alpha = 0.5)\n",
    "# alpha = 1.0\n",
    "NB_m2 = MultinomialNB(alpha = 1.0)\n",
    "# alpha = 1.5\n",
    "NB_m3 = MultinomialNB(alpha = 1.5)\n",
    "\n",
    "cv_results_NB_m1 = cross_validate(NB_m1, X_train, Y_train, return_train_score=True)\n",
    "cv_results_NB_m2 = cross_validate(NB_m2, X_train, Y_train, return_train_score=True)\n",
    "cv_results_NB_m3 = cross_validate(NB_m3, X_train, Y_train, return_train_score=True)\n",
    "\n",
    "NB_TrAcc = [cv_results_NB_m1['train_score'].mean(), cv_results_NB_m2['train_score'].mean(), cv_results_NB_m3['train_score'].mean()]\n",
    "NB_VaAcc = [cv_results_NB_m1['test_score'].mean(), cv_results_NB_m2['test_score'].mean(), cv_results_NB_m3['test_score'].mean()]\n",
    "\n",
    "print(\"Training accuracy:\", NB_TrAcc, \"\\nValidation accuracy:\", NB_VaAcc)\n",
    "\n",
    "NB_m1.fit(X_train, Y_train)\n",
    "Y_pred_m1 = NB_m1.predict(X_test)\n",
    "\n",
    "NB_m2.fit(X_train, Y_train)\n",
    "Y_pred_m2 = NB_m2.predict(X_test)\n",
    "\n",
    "NB_m3.fit(X_train, Y_train)\n",
    "Y_pred_m3 = NB_m3.predict(X_test)\n",
    "\n",
    "NB_TsAcc = [metrics.accuracy_score(Y_test, Y_pred_m1), metrics.accuracy_score(Y_test, Y_pred_m2), metrics.accuracy_score(Y_test, Y_pred_m3)]\n",
    "print(\"Testing accuracy\", NB_TsAcc)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
