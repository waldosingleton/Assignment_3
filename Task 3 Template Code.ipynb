{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BBC News Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Exploratory Data Analytics\n",
    "###### (a) Load the dataset and construct a feature vector for each article in the. You need to report the number of articles, and the number of extracted features. Show 5 example articles with their extracted features using a dataframe.\n",
    "###### (b) Conduct term frequency analysis and report three plots: (i) top-50 term frequency distribution across the entire dataset, (ii) term frequency distribution for respective class of articles, and (iii) class distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = pd.read_csv(\"train.csv\", skiprows=0, header=0, na_values= \"\", dtype=str)\n",
    "testDF = pd.read_csv(\"test.csv\", skiprows=0, header=0, na_values= \"\", dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 1\n",
      "article vector\n",
      " [[0 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "\n",
      "article vector (5 articles)\n",
      " [[0 0 0 ... 0 2 1]\n",
      " [3 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 1 1 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [2 1 1 ... 0 0 0]]\n",
      "\n",
      " Method 2\n",
      "article vector\n",
      " [[0.         0.02011467 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "\n",
      "article vector (5 articles)\n",
      " [[0.         0.         0.         ... 0.         0.08458453 0.05242018]\n",
      " [0.14037889 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.06456779 0.05209288 0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.10479134 0.06494308 0.06494308 ... 0.         0.         0.        ]]\n",
      "\n",
      "Articles: 428 , Extracted Features: 13518\n"
     ]
    }
   ],
   "source": [
    "articles_text = trainDF[\"Text\"].to_numpy()\n",
    "\n",
    "#select 5 random articles for task 1\n",
    "random_sample = random.sample(list(articles_text), 5)\n",
    "\n",
    "## APPROACH ONE ##\n",
    "vectorizer1 = CountVectorizer()\n",
    "vectorizer1.fit(articles_text)\n",
    "\n",
    "vectorizer1_sample = CountVectorizer()\n",
    "vectorizer1_sample.fit(random_sample)\n",
    "\n",
    "#Summary\n",
    "#print(f'vector vocabulary - {vectorizer.vocabulary_}\\n')\n",
    "\n",
    "# encode document\n",
    "vector1 = vectorizer1.transform(articles_text)\n",
    "vector1_sample = vectorizer1_sample.transform(random_sample)\n",
    "\n",
    "# summarize encoded vector\n",
    "print(\"Method 1\")\n",
    "print(f'article vector\\n {vector1.toarray()}')\n",
    "print(f'\\narticle vector (5 articles)\\n {vector1_sample.toarray()}')\n",
    "\n",
    "## APPROACH TWO ##\n",
    "vectorizer2 = TfidfVectorizer()\n",
    "vectorizer2.fit(articles_text)\n",
    "\n",
    "vectorizer2_sample = TfidfVectorizer()\n",
    "vectorizer2_sample.fit(random_sample)\n",
    "\n",
    "#Summary\n",
    "#print(f'vector vocabulary - {vectorizer.vocabulary_}\\n')\n",
    "\n",
    "# encode document\n",
    "vector2 = vectorizer2.transform(articles_text)\n",
    "vector2_sample = vectorizer2_sample.transform(random_sample)\n",
    "\n",
    "# summarize encoded vector\n",
    "print('\\n', \"Method 2\")\n",
    "print(f'article vector\\n {vector2.toarray()}')\n",
    "print(f'\\narticle vector (5 articles)\\n {vector2_sample.toarray()}')\n",
    "print('\\nArticles:', vector2.shape[0], ', Extracted Features:', vector2.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this code for Task 3(a). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.1_train</th>\n",
       "      <th>0.3_train</th>\n",
       "      <th>0.5_train</th>\n",
       "      <th>0.7_train</th>\n",
       "      <th>0.9_train</th>\n",
       "      <th>0.1_test</th>\n",
       "      <th>0.3_test</th>\n",
       "      <th>0.5_test</th>\n",
       "      <th>0.7_test</th>\n",
       "      <th>0.9_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LR</th>\n",
       "      <td>0.746269</td>\n",
       "      <td>0.99187</td>\n",
       "      <td>0.995434</td>\n",
       "      <td>0.996785</td>\n",
       "      <td>0.997403</td>\n",
       "      <td>0.596026</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.988764</td>\n",
       "      <td>0.989011</td>\n",
       "      <td>0.989011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NB</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.995434</td>\n",
       "      <td>0.996785</td>\n",
       "      <td>0.994819</td>\n",
       "      <td>0.884259</td>\n",
       "      <td>0.984026</td>\n",
       "      <td>0.985915</td>\n",
       "      <td>0.991736</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.957096</td>\n",
       "      <td>0.966825</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NN</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0.1_train 0.3_train 0.5_train 0.7_train 0.9_train  0.1_test  0.3_test  \\\n",
       "LR   0.746269   0.99187  0.995434  0.996785  0.997403  0.596026  0.977273   \n",
       "NB        1.0       1.0  0.995434  0.996785  0.994819  0.884259  0.984026   \n",
       "SVM       1.0       1.0       1.0       1.0       1.0  0.965517  0.957096   \n",
       "NN          0         0         0         0         0         0         0   \n",
       "\n",
       "     0.5_test  0.7_test  0.9_test  \n",
       "LR   0.988764  0.989011  0.989011  \n",
       "NB   0.985915  0.991736       1.0  \n",
       "SVM  0.966825  0.966667       1.0  \n",
       "NN          0         0         0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import floor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "\n",
    "m = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "training_output = pd.DataFrame(columns=m)\n",
    "testing_output = pd.DataFrame(columns=m)\n",
    "\n",
    "NUM_ARTICLES = vector1.shape[0]\n",
    "\n",
    "# LOGISTIC REGRESSION\n",
    "training_accuracies = {}\n",
    "testing_accuracies = {}\n",
    "\n",
    "# Transform the data into a format that can be used by the classifier\n",
    "text_transformer = TfidfVectorizer()\n",
    "X_train_text = text_transformer.fit_transform(trainDF['Text'])\n",
    "X_test_text = text_transformer.transform(testDF['Text'])\n",
    "\n",
    "for m_value in m:\n",
    "    TRAIN_LENGTH = floor(m_value * NUM_ARTICLES)\n",
    "    # Test how differing sizes of training set data affect test/train accuracy\n",
    "    X_train = X_train_text[0:TRAIN_LENGTH]  # First TRAIN_LENGTH articles\n",
    "    Y_train = trainDF['Category'][0:TRAIN_LENGTH] # First TRAIN_LENGTH categories\n",
    "\n",
    "    # Test Data Unchanged\n",
    "    X_test = X_test_text    # All test data text\n",
    "    Y_test = testDF['Category'] # All test data categories\n",
    "\n",
    "    # Train & Fit LR Model\n",
    "    logreg = LogisticRegression(penalty='l2', solver='lbfgs', C=m_value, multi_class='multinomial')\n",
    "    logreg.fit(X_train, Y_train)\n",
    "\n",
    "    F1_train_score = metrics.f1_score(Y_train, logreg.predict(X_train), pos_label = \"tech\")\n",
    "    F1_test_score = metrics.f1_score(Y_test, logreg.predict(X_test), pos_label = \"tech\")\n",
    "\n",
    "    # LR Model Accuracies\n",
    "    training_accuracies[m_value] = F1_train_score\n",
    "    testing_accuracies[m_value] = F1_test_score\n",
    "\n",
    "training_output = pd.concat(objs=[training_output, pd.DataFrame(training_accuracies, index=['LR'])])\n",
    "testing_output = pd.concat(objs=[testing_output, pd.DataFrame(testing_accuracies, index=['LR'])])\n",
    "\n",
    "# NAIVE BAYES\n",
    "training_accuracies = {}\n",
    "testing_accuracies = {}\n",
    "for m_value in m:\n",
    "    TRAIN_LENGTH = floor(m_value * NUM_ARTICLES)\n",
    "    VECTOR_ARRAY = vector1.toarray() # change this to vector 2 if needed\n",
    "    X_train = VECTOR_ARRAY[0:TRAIN_LENGTH]\n",
    "    X_test = VECTOR_ARRAY[TRAIN_LENGTH:]\n",
    "    Y_train = trainDF[\"Category\"].to_list()[:TRAIN_LENGTH]\n",
    "    Y_test = trainDF[\"Category\"].to_list()[TRAIN_LENGTH:]\n",
    "\n",
    "    # train your model here\n",
    "    NB_clf = MultinomialNB()\n",
    "    NB_clf.fit(X_train, Y_train)\n",
    "\n",
    "    # put your accuracy calc here\n",
    "    Y_train_pred = NB_clf.predict(X_train)\n",
    "    Y_pred = NB_clf.predict(X_test)\n",
    "    training_accuracies[m_value] = metrics.f1_score(Y_train, Y_train_pred, pos_label = \"tech\")\n",
    "    testing_accuracies[m_value] = metrics.f1_score(Y_test, Y_pred, pos_label = \"tech\")\n",
    "    \n",
    "training_output = pd.concat(objs=[training_output, pd.DataFrame(training_accuracies, index=['NB'])])\n",
    "testing_output = pd.concat(objs=[testing_output, pd.DataFrame(testing_accuracies, index=['NB'])])\n",
    "\n",
    "# SVM\n",
    "training_accuracies = {}\n",
    "testing_accuracies = {}\n",
    "for m_value in m:\n",
    "    TRAIN_LENGTH = floor(m_value * NUM_ARTICLES)\n",
    "    VECTOR_ARRAY = vector1.toarray()\n",
    "    X_train = VECTOR_ARRAY[0:TRAIN_LENGTH]\n",
    "    X_test = VECTOR_ARRAY[TRAIN_LENGTH:]\n",
    "    Y_train = trainDF[\"Category\"].to_list()[:TRAIN_LENGTH]\n",
    "    Y_test = trainDF[\"Category\"].to_list()[TRAIN_LENGTH:]\n",
    "\n",
    "    # train your model here\n",
    "    SVM_clf = svm.SVC(kernel='linear') \n",
    "    #'linear' gives better results than 'rbf', but not 100% sure yet. Im still tryna figure out Q2.\n",
    "    SVM_clf.fit(X_train, Y_train)\n",
    "    \n",
    "    # put your accuracy calc here\n",
    "    Y_train_pred = SVM_clf.predict(X_train)\n",
    "    Y_pred = SVM_clf.predict(X_test)\n",
    "    training_accuracies[m_value] = metrics.f1_score(Y_train, Y_train_pred, pos_label = \"tech\")\n",
    "    testing_accuracies[m_value] = metrics.f1_score(Y_test, Y_pred, pos_label = \"tech\")\n",
    "    \n",
    "training_output = pd.concat(objs=[training_output, pd.DataFrame(training_accuracies, index=['SVM'])])\n",
    "testing_output = pd.concat(objs=[testing_output, pd.DataFrame(testing_accuracies, index=['SVM'])])\n",
    "\n",
    "# Not Nearest Neighbour (lol)\n",
    "training_accuracies = {}\n",
    "testing_accuracies = {}\n",
    "for m_value in m:\n",
    "    TRAIN_LENGTH = floor(m_value * NUM_ARTICLES)\n",
    "    VECTOR_ARRAY = vector1.toarray() # change this to vector 2 if needed\n",
    "    X_train = VECTOR_ARRAY[0:TRAIN_LENGTH]\n",
    "    X_test = VECTOR_ARRAY[TRAIN_LENGTH:]\n",
    "    Y_train = trainDF[\"Category\"].to_list()[:TRAIN_LENGTH]\n",
    "    Y_test = trainDF[\"Category\"].to_list()[TRAIN_LENGTH:]\n",
    "\n",
    "    # train your model here\n",
    "\n",
    "    # put your accuracy calc here\n",
    "    training_accuracies[m_value] = 0\n",
    "    testing_accuracies[m_value] = 0\n",
    "    \n",
    "training_output = pd.concat(objs=[training_output, pd.DataFrame(training_accuracies, index=['NN'])])\n",
    "testing_output = pd.concat(objs=[testing_output, pd.DataFrame(testing_accuracies, index=['NN'])])\n",
    "\n",
    "pd.merge(training_output, testing_output, left_index=True, right_index=True, suffixes=('_train', '_test'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5(b)\n",
    "5-fold cross-validation to assess model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "04779b3cb1c73b27edf19ba4e819f6c1acf573bdf21c04e8831aec2965b440e8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 32-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
